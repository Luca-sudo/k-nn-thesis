#+title: Hypotheses

* Hypothesis 1

Conjecture: HNSW remains more precise than LSH on clustered data as the number of sites grows.

Method of evaluation: We generate two clusters, centered at $(-0.5, \dots, -0.5)$ and $(0.5, \dots, 0.5)$ respectively. These are hypercubes with a diameter of $0.4$. Put differently, for all sites $s = (s_1, \dots, s_d)$ in the first cluster, it holds that $-0.7 \leq s_i \leq -0.3$. For the second cluster this corresponds to $0.3 \leq s_i \leq 0.7$. Note that sites within each cluster are sampled uniformly.

For successive instances, the number of sites is the only parameter that is mutated, increasing from 10000 overall sites across both clusters to $100000$.

As is common with all test for LSH and HNSW, we evaluate the /quality/ of the solution. We define the quality of a candidate solution $\text{cand}$ relative to the optimal solution $\text{opt}$. The quality is given by $\frac{opt}{cand}$, which is equal to $1$ if the candidate solution conincides with the optimal solution. Otherwise, the quality degrades towards $0$ as the distance to the candidate solution increases relative to the optimal solution.

For the implementation: Of course, the usual preamble that imports required modules, defines a target path for the data to be generated, and seeds the RNG is required. For those running the script from the CLI, we also provide the hypothesis and a description of the test.

#+begin_src python :tangle hypothesis_1.py
#!/usr/bin/env python3

# Hypothesis 1: HNSW remains more precise than LSH on clustered data as the number of sites grows.

import time
import numpy as np
import h5py
import faiss

filepath = "data/hypothesis_1"

np.random.seed(42)

hypothesis = "HNSW remains more precise than LSH on clustered data as the number of sites grows."

description = '''
We generate two clusters, centered at $(-0.5, \dots, -0.5)$ and $(0.5, \dots, 0.5)$ respectively. These are hypercubes with a diameter of $0.4$. Put differently, for all sites $s = (s_1, \dots, s_d)$ in the first cluster, it holds that $-0.7 \leq s_i \leq -0.3$. For the second cluster this corresponds to $0.3 \leq s_i \leq 0.7$. Note that sites within each cluster are sampled uniformly.

'''
#+end_src

Afterwards, we define all instances through lists of parameters: the number of sites (n_sites), the dimensionality (n_dims), and the number of nearest neighbors to compute (k).

#+begin_src python :tangle hypothesis_1.py
n_sites = [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]
n_dims = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]
k = 5
#+end_src

We then check that we have equally many values for n_sites and n_dims. Otherwise, we forgot to define our instances appropriately. Following this check, we already write the parameter k and the number of instances to our target file. We also write the brief description and hypothesis to the file already.

#+begin_src python :tangle hypothesis_1.py
assert(len(n_sites) == len(n_dims))

file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_instances'] = len(n_sites)
file.attrs['description'] = description
file.attrs['hypothesis'] = hypothesis
#+end_src

What follows now is the actual generation of instance data.

#+begin_src python :tangle hypothesis_1.py
for i in range(len(n_sites)):
#+end_src

For this hypothesis, we generate two clusters centered around $(-0.5, \dots, -0.5)$ and $(0.5, \dots, 0.5)$, and time this generation.

#+begin_src python :tangle hypothesis_1.py
    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    first_cluster = np.random.uniform(-0.7, -0.3, (int(n_sites[i] / 2), n_dims[i]))
    second_cluster = np.random.uniform(0.3, 0.7, (int(n_sites[i] / 2), n_dims[i]))
    sites = first_cluster + second_cluster
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')
#+end_src

We similarly generate a random query.

#+begin_src python :tangle hypothesis_1.py
    query = np.random.uniform(-1.0, 1.0, (1, n_dims[i]))
#+end_src

Afterwards, we generate the flat index -- that is, an index that linearly scans -- on the sites and query against it. Its exhaustiveness guarantees detection of optimal solutions.

#+begin_src python :tangle hypothesis_1.py
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims[i])
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')
    time_start = time.perf_counter()
    distance, solution = index.search(query, k)
    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')
#+end_src

Finally, we write all the instance data to file under the dataset instance_i. This includes the number of sites generated, the dimensionality of the feature space, the query, as well as the solution.

#+begin_src python :tangle hypothesis_1.py
    instance = file.create_dataset('instance_' + str(i), data=sites)
    instance.attrs['n_sites'] = n_sites[i]
    instance.attrs['n_dims'] = n_dims[i]
    file.create_dataset('query_' + str(i), data=query)
    file.create_dataset('solution_' + str(i), data=solution)
    file.create_dataset('distance_' + str(i), data=distance)
#+end_src

The test data can be generated with the associated Makefile, too. Run `make generate_hypothesis_1` for that.
