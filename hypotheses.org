#+title: Hypotheses

This file serves as a literate program for the data generation of all hypotheses. Each section states the hypothesis, describes the test instance and explains portions of the source code. Note that all .py files of the form `hypothesis_i.py` are generated from this file using the babel extension for emacs.

A literate program testing the generated data and evaluating aspects such as quality, performance, etc. can be found at `test_lsh_hnsw.org`. It, too, motivates portions of the source code.

Before diving into the hypotheses, let us sketch two parts of data generation that are shared across the source code of all hypotheses: imports and the testing loop. Imports simply comprise all libraries that are shared across the data generation for all hypotheses, as well as a function `invert`, which we use to lookup the rank of a site.


#+NAME: preamble
#+BEGIN_SRC python :tangle no
#!/usr/bin/env python3
import time
import numpy as np
import h5py
import faiss

# Given a list that contains the ids of sites, invert populates a new list where the value at the id is the rank of the site.
# If, for example, the site with id 5 was the nearest neighbor, then the resulting list would have value 1 at position 5.
# This way, computing the ranks of candidate solutions is possible in constant time, at the cost of some memory.
def invert(l):
    new_l = [0 for i in range(len(l))]

    for index, value in enumerate(l):
        new_l[value] = index + 1

    return new_l
#+END_SRC

The testing loop is more interesting. It starts of by defining key metadata about the data we generate. This is important when the data is tested in `test_lsh_hnsw.org`, as we have to know the structure of the data we deal with (i.e. how many sites are there, what is the dimensionality, etc).

The for loop then generates the data and computes solutions for each instance. There are parts here which will be defined by the hypotheses themselves, such as `site_generator()` and `query_generator`. These functions simply generate the set of sites, and queries, for the specific instance. The remaining code inside of the for loop constructs a flat index for brute-force, orders all sites by their proximity to the query point, and stores results for each instance.

#+NAME: test
#+BEGIN_SRC python :tangle no
file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_dims'] = n_dims
file.attrs['n_sites'] = n_sites
file.attrs['n_planes'] = n_planes
file.attrs['n_instances'] = len(n_sites)
file.attrs['description'] = description
file.attrs['hypothesis'] = hypothesis
file.attrs['sample_size'] = sample_size

for i in range(len(n_sites)):
    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    sites = site_generator(i)
    sites = np.array(sites, dtype=np.float32)
    print(f'sites: {sites}')
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')
    queries = query_generator(i)
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims[i])
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')

    time_start = time.perf_counter()
    distance, solution = index.search(queries, n_sites[i])

    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')

    k_nearest = list(map(lambda x: x[:k[i]], solution))
    ranks = solution
    print(ranks[0])
    print(len(ranks[0]))

    ranks = list(map(invert, ranks))

    site_to_distance = []
    for sample_idx in range(sample_size):
        temp = [0.0 for i in range(n_sites[i])]
        for j in range(len(temp)):
            site_idx = solution[sample_idx][j]
            temp[site_idx] = distance[sample_idx][j]
        site_to_distance.append(temp)

    instance = file.create_dataset('instance_' + str(i), data=sites)
    file.create_dataset('queries_' + str(i), data=queries)
    file.create_dataset('solution_' + str(i), data=k_nearest)
    file.create_dataset('ranks_' + str(i), data = ranks)
    file.create_dataset('distance_' + str(i), data=site_to_distance)
#+END_SRC

The data-defining part of each hypothesis will live between these two shared components; after the imports and before the testing loop. This way, we avoid duplication and have a declarative definition of the instances for each hypothesis.

* Hypothesis 1

Conjecture: HNSW remains more precise than LSH on clustered data as the number of sites grows.

Method of evaluation: We generate two clusters, centered at $(-0.5, \dots, -0.5)$ and $(0.5, \dots, 0.5)$ respectively. These are hypercubes with a diameter of $0.4$. Put differently, for all sites $s = (s_1, \dots, s_d)$ in the first cluster, it holds that $-0.7 \leq s_i \leq -0.3$. For the second cluster this corresponds to $0.3 \leq s_i \leq 0.7$. Note that sites within each cluster are sampled uniformly.

For successive instances, the number of sites is the only parameter that is mutated, increasing from 10000 overall sites across both clusters to $100000$.

As is common with all test for LSH and HNSW, we evaluate the /quality/ of the solution. We define the quality of a candidate solution $\text{cand}$ relative to the optimal solution $\text{opt}$. The quality is given by $\frac{opt}{cand}$, which is equal to $1$ if the candidate solution conincides with the optimal solution. Otherwise, the quality degrades towards $0$ as the distance to the candidate solution increases relative to the optimal solution.

For the implementation: Of course, the usual preamble that imports required modules, defines a target path for the data to be generated, and seeds the RNG is required. For those running the script from the CLI, we also provide the hypothesis and a description of the test.

#+begin_src python :tangle hypotheses/hypothesis_1.py :noweb yes
<<preamble>>

# Hypothesis 1: HNSW remains more precise than LSH on clustered data as the number of sites grows.
filepath = "data/hypothesis_1.h5"

np.random.seed(42)

hypothesis = "HNSW remains more precise than LSH on clustered data as the number of sites grows."

description = '''
We generate two clusters, centered at $(-0.5, \dots, -0.5)$ and $(0.5, \dots, 0.5)$ respectively. These are hypercubes with a diameter of $0.4$. Put differently, for all sites $s = (s_1, \dots, s_d)$ in the first cluster, it holds that $-0.7 \leq s_i \leq -0.3$. For the second cluster this corresponds to $0.3 \leq s_i \leq 0.7$. Note that sites within each cluster are sampled uniformly.
'''

n_sites = [2**i for i in range(11, 23)]
n_dims = [100 for i in n_sites]
n_planes = [dim * 2 for dim in n_dims]
k = [5 for i in n_sites]
sample_size = 20
site_generator = lambda i: np.random.uniform(-0.7, -0.3, (int(n_sites[i] / 2), n_dims[i])) + np.random.uniform(0.3, 0.7, (int(n_sites[i] / 2), n_dims[i]))
query_generator = lambda i: np.random.uniform(-1.0, 1.0, (sample_size, n_dims[i]))

<<test>>

file.attrs['var_name'] = "Cluster Size"
file.attrs['var_values'] = list(map(lambda x: x / 2, n_sites))
#+end_src



* Hypothesis 2

Conjecture: Given two clusters in the upper-right quadrant of cartesian space, HNSW's quality remains steady and LSH's quality improves as the distance between the two clusters grows. The distance between clusters directly corresponds to the spread of the underlying distribution -- some pairs of points remain infinitesimally close to each other, while the distance between points of separate clusters is directly affected by the distance between the two clusters.

Method of evaluation: All generated instances exhibit the same number of overall sites, distributed equally aross both clusters. The only parameter that changes between instances is the distance between the center points of both clusters. This distance increases exponentially, given by the formula $2^i$ for instance $i$.

We begin with the usual preamble, importin modules, declaring target filepaths for to-be-generated data, etc.

#+begin_src python :tangle hypotheses/hypothesis_2.py :noweb yes
<<preamble>>

# Hypothesis 2: HNSW quality remains steady while LSH quality improves with increased spread (due to higher cosine similarity).

filepath = "data/hypothesis_2.h5"

hypothesis = "HNSW quality remains steady while LSH quality increases with growing spread."

description = """
This test generates two clusters in the upper-right quadrant of the coordinate system.
The center points of the clusters are chosen to be $1.0$ and $1.0 + spread$ respectively.
Both clusters allow for points within -0.2 and 0.2 range across all axes.
"""

np.random.seed(42)

spreads = [2.0 ** i for i in range(6, 50)]
n_sites = [1000 for i in range(len(spreads))]
n_dims = [100 for i in range(len(spreads))]
n_planes = [2 * dim for dim in n_dims]
k = [5 for i in range(len(spreads))]
sample_size = 20
first_center = [3.0 for i in spreads]
second_center = [first_center[i] + spreads[i] for i in range(len(spreads))]
site_generator = lambda i: np.random.uniform(first_center[i] - 2.0, first_center[i] + 2.0, (int(n_sites[i] / 2), n_dims[i])) + np.random.uniform(second_center[i] - 2.0, second_center[i] + 2.0, (int(n_sites[i] / 2), n_dims[i]))
query_generator = lambda i: np.random.uniform(0.0, second_center[i] + 2.0, (sample_size, n_dims[i]))

<<test>>

file.attrs['var_name'] = "Spread"
file.attrs['var_values'] = spreads
#+end_src

The data can be generated using `make data/hypothesis_2`.

* Hypothesis 3

Hypothesis: On a uniform grid (all sites have integer coordinates within a bounded region), HNSW retains quality whereas LSH degrades in quality as the size of the region is increased.

Method of evaluation: We generate a set of integer-coordinate sites within the two-dimensional square with some extent. This square has its lower-left corner at the origin. Across instances, the extent -- i.e. the region -- is increased horizontally and vertically. Every other parameter remains fixed throughout.

#+BEGIN_SRC python :tangle hypotheses/hypothesis_3.py :noweb yes
<<preamble>>

# Hypothesis 3: HNSW remains precise on a uniform grid, whereas LSH degenerates due to cosine similarity collisions.
filepath = "data/hypothesis_3.h5"

hypothesis = "HNSW remains precise on a uniform grid, whereas LSH degenerates due to cosine similarity collisions."

description = """
This test generates a two-dimensional lattice with fixed extents.
To this extent, all sites have the form $(i, j)$ with $i, j \in \mathbb{N}$ and $i, j \leq \\text{extent}$.
"""

np.random.seed(42)

# Define all relevant data
extents = [(2 ** i) for i in range(4, 10)]
n_sites = [(extents[i] ** 2) for i in range(len(extents))]
n_dims = [2 for i in range(len(extents))]
n_planes = [(2 * dim) for dim in n_dims]
k = [5 for i in range(len(extents))]
sample_size = 20
site_generator = lambda i: [(np.float64(x),np.float64(y)) for x in range(extents[i]) for y in range(extents[i])]
query_generator = lambda i: np.random.uniform(0.0, extents[i] - 1.0, (sample_size, n_dims[i]))

<<test>>

file.attrs['var_name'] = "Extent"
file.attrs['var_values'] = extents
#+END_SRC




* Hypothesis 4

Hypothesis: The observed loss of quality in hypothesis 3 can /not/ be counteracted by increasing the number of separating hyperplanes.

Method of Evaluation: Consider a uniform grid, akin to hypothesis 3, but this time with a fixed extent. Successive instances increase the number of separating hyperplanes.


#+begin_src python :tangle hypotheses/hypothesis_4.py :noweb yes
<<preamble>>

# Hypothesis 4: The observed loss of quality in hypothesis 3 can /not/ be counteracted by increasing the number of separating hyperplanes.

extent = 100
n_dims = [2 for i in range(20)]
n_planes = [i * n_dims[i] for i in range(len(n_dims))]
n_sites = [extent ** n_dims[i] for i in range(len(n_dims))]
k = [5 for i in range(len(n_dims))]
sample_size = 20
filepath = "data/hypothesis_4.h5"
site_generator = lambda i: [(x,y) for x in range(extent) for y in range(extent)]
query_generator = lambda i: np.random.uniform(0.0, extent, (sample_size, n_dims[i]))

hypothesis = "The observed loss of quality in hypothesis 3 can /not/ be counteracted by increasing the number of separating hyperplanes."

description = """
Consider a uniform grid, akin to hypothesis 3, but this time with a fixed extent. Successive instances increase the number of separating hyperplanes.
"""

np.random.seed(42)

<<test>>

file.attrs['var_name'] = "Nr. of Hyperplanes"
file.attrs['var_values'] = n_planes
#+end_src

* Hypothesis 5

Hypothesis: Over a uniform-grid similar to hypotheses 3 & 4, the quality of LSH diminishes as the dimensionality of the feature space increases over a bounded region.

Method of evaluation: Successive instances share the extent of the bounded region inhabited by the set of sites. However, the dimensionality increments for successive instances.

Note that this hypothesis requires the library `itertools` in order to generate all sites of the uniform grid for variable dimensions.

#+begin_src python :tangle hypotheses/hypothesis_5.py :noweb yes
import itertools
<<preamble>>

# Hypothesis 5: Over a uniform-grid similar to hypotheses 3 & 4, the quality of LSH diminishes as the dimensionality of the feature space increases over a bounded region.

extent = 5
n_dims = [i for i in range(2, 10)]
n_planes = [dim * 2 for dim in n_dims]
n_sites = [5 ** dim for dim in n_dims]
k = [5 for i in range(len(n_dims))]
sample_size = 20
site_generator = lambda i: [s for s in itertools.product(range(extent), repeat=n_dims[i])]
query_generator = lambda i: np.random.uniform(0, extent, (sample_size, n_dims[i]))

filepath = "data/hypothesis_5.h5"

hypothesis = "Over a uniform-grid similar to hypotheses 3 & 4, the quality of LSH diminishes as the dimensionality of the feature space increases."

description = """
Consider a uniform grid, akin to hypothesis 3 & 4, but this time the extent is fixed and the dimensionality increase for successive instances.
"""

np.random.seed(42)

<<test>>

file.attrs['var_name'] = "Dimension"
file.attrs['var_values'] = n_dims
#+end_src

* Hypothesis 6

Hypothesis: Quality of LSH queries degenerates as density of sites increases.

Method of evaluation: We generate a uniformly-distributed set of sites inside of the unit hypercube centered around the origin. Successive instances increase the number of sites linearly.

#+begin_src python :tangle hypotheses/hypothesis_6.py :noweb yes
<<preamble>>

# Hypothesis 6: Quality of LSH queries degenerates as density of sites increases.

n_sites = [2 ** i for i in range(10, 20)]
n_dims = [100 for i in range(len(n_sites))]
n_planes = [dim * 2 for dim in n_dims]
k = [5 for i in range(len(n_sites))]
sample_size = 20
filepath = "data/hypothesis_6.h5"
site_generator = lambda i: np.random.uniform(-1.0, 1.0, (n_sites[i], n_dims[i]))
query_generator =  lambda i: np.random.uniform(-1.0, 1.0, (sample_size, n_dims[i]))

hypothesis = "Quality of LSH queries degenerates as density of sites increases."

description = """
We generate a uniformly-distributed set of sites inside of the unit hypercube centered around the origin. Successive instances increase the number of sites linearly.
"""

np.random.seed(42)

<<test>>

file.attrs['var_name'] = "Nr. of Sites"
file.attrs['var_values'] = n_sites

#+end_src

* Hypothesis 7

We saw that for hypothesis 3 the quality of LSH was hamstringed, mainly due to the colinearity of sites. This hypothesis evaluates the effectiveness of LSH on a grid that does not suffer from colinearity to the same degree. To this extent, the grid from hypothesis 3 is translated by an irrational number. This way, the probability of two sites being co-linear is drastically reduced.

We conjecture that LSH-quality remains steady on this data set, providing a desirable recall of $>90%$.


#+begin_src python :tangle hypotheses/hypothesis_7.py :noweb yes
<<preamble>>

# Hypothesis 7: LSH-quality and recall are servicable on a grid translated by an irrational number, because co-linearity of sites is curbed.


extents = [5, 10, 20, 30, 40, 50, 75, 100, 200, 300, 500]
n_dims = [2 for i in extents]
n_planes = [2 * dim for dim in n_dims]
n_sites = [extents[i] ** n_dims[i] for i in range(len(extents))]
k = [5 for i in extents]
sample_size = 20
filepath = "data/hypothesis_7.h5"
site_generator = lambda x: [(x + np.pi, y + np.pi) for x in range(extents[i]) for y in range(extents[i])]
query_generator = lambda x: np.random.uniform(np.pi, extents[i] + np.pi, (sample_size, n_dims[i]))

hypothesis = "LSH-quality and recall are servicable on a grid translated by an irrational number, because co-linearity of sites is curbed."

description = """
This test generates a two-dimensional lattice with fixed extents that is translated by $\pi$.
To this extent, all sites have the form $(i + \pi, j + \pi)$ with $i, j \in \mathbb{N}$ and $i, j \leq \\text{extent}$.
"""

np.random.seed(42)

<<test>>

file.attrs['var_name'] = "Extent"
file.attrs['var_values'] = extents
#+end_src

* Hypothesis 8

This hypothesis aims to provide a dataset that, in theory, should favor the characteristic of LSH. Since the collision probability in the underlying hash table depends on the cosine similarity of sites, sampling points on the unit sphere should provide an optimal setting for LSH. This is exactly what is done: successive instances sample an increasing number of points on the unit-sphere.

We, therefore, conjecture that both quality and recall are excellent on data distributed on the unit sphere, even as the number of sites increases.


#+begin_src python :tangle hypotheses/hypothesis_8.py
#!/usr/bin/env python3

# Hypothesis 8: LSH-quality and recall are desirable on data distributed on the unit sphere.

import time
import numpy as np
import h5py
import faiss

n_dims = 100
n_sites = [2 ** i for i in range (10, 20)]
k = 5
sample_size = 20
filepath = "data/hypothesis_8.h5"

hypothesis = "LSH-quality and recall are desirable on data distributed on the unit sphere."

description = """
This dataset is generated by uniformly sampling radians $d-1$ times. These angles are then used to calculate the coordinate representation of the associated vector on the unit sphere.
"""

np.random.seed(42)

file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_instances'] = len(n_sites)
file.attrs['description'] = description
file.attrs['hypothesis'] = hypothesis
file.attrs['sample_size'] = sample_size
file.attrs['var_name'] = "Nr. of Sites"
file.attrs['var_values'] = n_sites

# Turn d-1 angles into a d-dimensional coordinate vector.
def rad_to_coordinate(angles):
    coordinates = []
    # Precompute sines and cosines
    sines = np.sin(angles)
    cosines = np.cos(angles)
    for i in range(len(angles) + 1):
        x = 1
        for j in range(i):
            x *= sines[j]
        if i < len(angles):
            x *= cosines[i]
        coordinates.append(x)
    return coordinates


for i in range(len(n_sites)):
    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    # d - 1 many angles required for the d-dimensional representation
    angles = np.random.uniform(0, 2.0 * np.pi, (n_sites[i], n_dims - 1))
    sites = list(map(rad_to_coordinate, angles))
    sites = np.array(sites, dtype=np.float32)
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')
    query_angles = np.random.uniform(0, 2.0 * np.pi, (sample_size, n_dims - 1))
    queries = np.array(list(map(rad_to_coordinate, query_angles)), dtype=np.float32)
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims)
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')

    time_start = time.perf_counter()
    distance, solution = index.search(queries, n_sites[i])
    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')

    k_nearest = list(map(lambda x: x[:k], solution))
    ranks = solution
    k_nearest_distances = list(map(lambda x: x[:k], distance))

    def invert(l):
        new_l = [0 for i in range(len(l))]

        for index, value in enumerate(l):
            new_l[value] = index + 1

        return new_l

    ranks = list(map(invert, ranks))

    instance = file.create_dataset('instance_' + str(i), data=sites)
    instance.attrs['n_sites'] = n_sites
    instance.attrs['n_dims'] = n_dims
    instance.attrs['n_planes'] = n_dims * 2
    file.create_dataset('queries_' + str(i), data=queries)
    file.create_dataset('solution_' + str(i), data=k_nearest)
    file.create_dataset('ranks_' + str(i), data = ranks)
    file.create_dataset('distance_' + str(i), data=k_nearest_distances)
#+end_src

* Footnotes

[fn:1] Distant referring to the breaking point from the previous hypothesis, meaning distances at which point the specific configuration skews in favor of LSH.
