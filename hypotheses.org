#+title: Hypotheses

This file serves as a literate program for the data generation of all hypotheses. Each section states the hypothesis, describes the test instance and explains portions of the source code. Note that all .py files of the form `hypothesis_i.py` are generated from this file using the babel extension for emacs.

A literate program testing the generated data and evaluating aspects such as quality, performance, etc. can be found at `test_lsh_hnsw.org`. It, too, motivates portions of the source code.

* Hypothesis 1

Conjecture: HNSW remains more precise than LSH on clustered data as the number of sites grows.

Method of evaluation: We generate two clusters, centered at $(-0.5, \dots, -0.5)$ and $(0.5, \dots, 0.5)$ respectively. These are hypercubes with a diameter of $0.4$. Put differently, for all sites $s = (s_1, \dots, s_d)$ in the first cluster, it holds that $-0.7 \leq s_i \leq -0.3$. For the second cluster this corresponds to $0.3 \leq s_i \leq 0.7$. Note that sites within each cluster are sampled uniformly.

For successive instances, the number of sites is the only parameter that is mutated, increasing from 10000 overall sites across both clusters to $100000$.

As is common with all test for LSH and HNSW, we evaluate the /quality/ of the solution. We define the quality of a candidate solution $\text{cand}$ relative to the optimal solution $\text{opt}$. The quality is given by $\frac{opt}{cand}$, which is equal to $1$ if the candidate solution conincides with the optimal solution. Otherwise, the quality degrades towards $0$ as the distance to the candidate solution increases relative to the optimal solution.

For the implementation: Of course, the usual preamble that imports required modules, defines a target path for the data to be generated, and seeds the RNG is required. For those running the script from the CLI, we also provide the hypothesis and a description of the test.

#+begin_src python :tangle hypotheses/hypothesis_1.py
#!/usr/bin/env python3

# Hypothesis 1: HNSW remains more precise than LSH on clustered data as the number of sites grows.

import time
import numpy as np
import h5py
import faiss

filepath = "data/hypothesis_1"

np.random.seed(42)

hypothesis = "HNSW remains more precise than LSH on clustered data as the number of sites grows."

description = '''
We generate two clusters, centered at $(-0.5, \dots, -0.5)$ and $(0.5, \dots, 0.5)$ respectively. These are hypercubes with a diameter of $0.4$. Put differently, for all sites $s = (s_1, \dots, s_d)$ in the first cluster, it holds that $-0.7 \leq s_i \leq -0.3$. For the second cluster this corresponds to $0.3 \leq s_i \leq 0.7$. Note that sites within each cluster are sampled uniformly.
'''
#+end_src

Afterwards, we define all instances through lists of parameters: the number of sites (n_sites), the dimensionality (n_dims), and the number of nearest neighbors to compute (k).

#+begin_src python :tangle hypotheses/hypothesis_1.py
n_sites = [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]
n_dims = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]
k = 5
sample_size = 20
#+end_src

We then check that we have equally many values for n_sites and n_dims. Otherwise, we forgot to define our instances appropriately. Following this check, we already write the parameter k and the number of instances to our target file. We also write the brief description and hypothesis to the file already.

#+begin_src python :tangle hypotheses/hypothesis_1.py
assert(len(n_sites) == len(n_dims))

file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_instances'] = len(n_sites)
file.attrs['description'] = description
file.attrs['hypothesis'] = hypothesis
file.attrs['sample_size'] = sample_size
#+end_src

What follows now is the actual generation of instance data.

#+begin_src python :tangle hypotheses/hypothesis_1.py
for i in range(len(n_sites)):
#+end_src

For this hypothesis, we generate two clusters centered around $(-0.5, \dots, -0.5)$ and $(0.5, \dots, 0.5)$, and time this generation.

#+begin_src python :tangle hypotheses/hypothesis_1.py
    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    first_cluster = np.random.uniform(-0.7, -0.3, (int(n_sites[i] / 2), n_dims[i]))
    second_cluster = np.random.uniform(0.3, 0.7, (int(n_sites[i] / 2), n_dims[i]))
    sites = first_cluster + second_cluster
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')
#+end_src

We similarly generate a random query.

#+begin_src python :tangle hypotheses/hypothesis_1.py
    queries = np.random.uniform(-1.0, 1.0, (sample_size, n_dims[i]))
#+end_src

Afterwards, we generate the flat index -- that is, an index that linearly scans -- on the sites and query against it. Its exhaustiveness guarantees detection of optimal solutions. Note that we pass all query vectors to the `index.search()` function for batch querying. Both results are now lists of solutions for each of the query vectors.

#+begin_src python :tangle hypotheses/hypothesis_1.py
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims[i])
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')
    time_start = time.perf_counter()
    distance, solution = index.search(queries, int(n_sites[i]))
    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')
    print(f'\tDistances: {distance}')
    print(f'\tSolutions: {solution}')
#+end_src

Finally, we write all the instance data to file under the dataset instance_i. This includes the number of sites generated, the dimensionality of the feature space, the query, as well as the solution. Note that we computed the exact ordering of all sites with respect to the nearest neighbor query, also called the /rank/. In order to determine, for a given sites $s$, what neighbor it is to the query, one needs to search the `ranks_i` list.

Note that we materialize the three different views into the solution list, since supplying `file.create_dataset(..., data = solution[0][:k])` evaluates to the raw pointer, storing the solution list itself.

#+begin_src python :tangle hypotheses/hypothesis_1.py
    k_nearest = list(map(lambda x: x[:k], solution))
    ranks = solution
    k_nearest_distances = list(map(lambda x: x[:k], distance))
#+end_src

In order to accelerate determination of the rank $i$ of a site $s$, we invert the role of indices and values in the `ranks` list. Then, indexing into this list with the appropriate site $s$, which is merely an id, yields the desired $i$ in constant time. Otherwise, the list would need to be searched for the site $s$, drastically decreasing speed. Note that we increment the index by one. This ensures that ranks start at value one, which is important for plotting later on.

#+begin_src python :tangle hypotheses/hypothesis_1.py
    def invert(l):
        new_l = [0 for i in range(len(l))]

        for index, value in enumerate(l):
            new_l[value] = index + 1

        return new_l

    ranks = list(map(invert, ranks))
#+end_src


#+begin_src python :tangle hypotheses/hypothesis_1.py
    instance = file.create_dataset('instance_' + str(i), data=sites)
    instance.attrs['n_sites'] = n_sites[i]
    instance.attrs['n_dims'] = n_dims[i]
    instance.attrs['n_planes'] = n_dims[i] * 2
    file.create_dataset('queries_' + str(i), data=queries)
    file.create_dataset('solution_' + str(i), data=k_nearest)
    file.create_dataset('ranks_' + str(i), data = ranks)
    file.create_dataset('distance_' + str(i), data=k_nearest_distances)
#+end_src

The test data can be generated with the associated Makefile, too. Run `make data/hypothesis_1` for that.

* Hypothesis 2

Conjecture: Given two clusters in the upper-right quadrant of cartesian space, HNSW's quality remains steady and LSH's quality improves as the distance between the two clusters grows. The distance between clusters directly corresponds to the spread of the underlying distribution -- some pairs of points remain infinitesimally close to each other, while the distance between points of separate clusters is directly affected by the distance between the two clusters.

Method of evaluation: All generated instances exhibit the same number of overall sites, distributed equally aross both clusters. The only parameter that changes between instances is the distance between the center points of both clusters. This distance increases exponentially, given by the formula $2^i$ for instance $i$.

We begin with the usual preamble, importin modules, declaring target filepaths for to-be-generated data, etc.

#+begin_src python :tangle hypotheses/hypothesis_2.py
#!/usr/bin/env python3

# Hypothesis 2: HNSW quality remains steady while LSH quality improves with increased spread (due to higher cosine similarity).

import time
import numpy as np
import h5py
import faiss

filepath = "data/hypothesis_2"

hypothesis = "HNSW quality remains steady while LSH quality increases with growing spread."

description = """
This test generates two clusters in the upper-right quadrant of the coordinate system.
The center points of the clusters are chosen to be $-spread / 2.0$ and $spread / 2.0$ respectively.
Both clusters allow for points within -0.2 and 0.2 range across all axes.
"""

np.random.seed(42)

n_sites = 100000
n_dims = 100
k = 5
#+end_src

Finally, we define a list of spreads, which correspond to the aforementioned distance between the centers cluster points.

#+begin_src python :tangle hypotheses/hypothesis_2.py
# This includes spreads up until (and including) $2^{20}$.
spreads = [2.0 ** i for i in range(21)]
#+end_src

We then write attributes shared across all instances to the file and begin generating data for each instance.

#+begin_src python :tangle hypotheses/hypothesis_2.py
file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_instances'] = len(spreads)
file.attrs['hypothesis'] = hypothesis
file.attrs['description'] = description

for i in range(len(spreads)):
#+end_src

The first cluster is centered at $-2^{i-1}$, while the second cluster is centered at $2^{i - 1}$. Within each cluster, the sites are sampled uniformly, akin to the first hypothesis. Each clusters forms a hypoercube of diameter $0.4$. Our entire set of sites is made up of the first cluster and the second cluster.

#+begin_src python :tangle hypotheses/hypothesis_2.py
    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    first_center = -(spreads[i] / 2.0)
    second_center = (spreads[i] / 2.0)
    first_cluster = np.random.uniform(first_center - 0.2, first_center + 0.2, (int(n_sites / 2), n_dims)) - 0.7
    second_cluster = np.random.uniform(second_center - 0.2, second_center + 0.2, (int(n_sites / 2), n_dims)) + 0.3
    sites = first_cluster + second_cluster
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')
#+end_src

The query to benchmark is drawn uniformly too, but it samples coordinate values between $-2^{i-1}$ and $2^{i-1}$. This ensures that the query lies between both clusters and not inside one of them.

#+begin_src python :tangle hypotheses/hypothesis_2.py
    query = np.random.uniform(first_center, second_center, (1, n_dims))
#+end_src

Finally, we create a flat index for exhaustive search, add the sites and compute the optimal k-nearest neighbors. All of which is stored in the file, before we move on to the next instance and repeat.

#+begin_src python :tangle hypotheses/hypothesis_2.py
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims)
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')
    time_start = time.perf_counter()
    distance, solution = index.search(query, n_sites)
    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')

    k_nearest = solution[0][:k]
    ranks = solution[0]
    k_nearest_distances = distance[0][:k]

    def invert(l):
        new_l = [0 for i in range(len(l))]

        for index, value in enumerate(l):
            new_l[value] = index

        return new_l

    ranks = invert(ranks)


    instance = file.create_dataset('instance_' + str(i), data=sites)
    instance.attrs['n_sites'] = n_sites
    instance.attrs['n_dims'] = n_dims
    instance.attrs['n_planes'] = n_dims * 2
    file.create_dataset('query_' + str(i), data=query)
    file.create_dataset('solution_' + str(i), data=k_nearest)
    file.create_dataset('ranks_' + str(i), data = ranks)
    file.create_dataset('distance_' + str(i), data=k_nearest_distances)
#+end_src

The data can be generated using `make data/hypothesis_2`.

* Hypothesis 3

Hypothesis: On a uniform grid (all sites have integer coordinates within a bounded region), HNSW retains quality whereas LSH degrades in quality as the size of the region is increased.

Method of evaluation: We generate a set of integer-coordinate sites within the two-dimensional square with some extent. This square has its lower-left corner at the origin. Across instances, the extent -- i.e. the region -- is increased horizontally and vertically. Every other parameter remains fixed throughout.

We begin with the usual preamble:

#+begin_src python :tangle hypotheses/hypothesis_3.py
#!/usr/bin/env python3

# Hypothesis 3: HNSW remains precise on a uniform grid, whereas LSH degenerates due to cosine similarity collisions.

import time
import numpy as np
import h5py
import faiss

n_dims = 2
k = 5
extents = [5, 10, 20, 30, 40, 50, 75, 100, 200, 300, 500]
filepath = "data/hypothesis_3"

hypothesis = "HNSW remains precise on a uniform grid, whereas LSH degenerates due to cosine similarity collisions."

description = """
This test generates a two-dimensional lattice with fixed extents.
To this extent, all sites have the form $(i, j)$ with $i, j \in \mathbb{N}$ and $i, j \leq \\text{extent}$.
"""

np.random.seed(42)

file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_instances'] = len(extents)
file.attrs['description'] = description
file.attrs['hypothesis'] = hypothesis

for i in range(len(extents)):
#+end_src

Afterwards, we generate the set of sites. To reiterate: this set comprises /all/ integer-coordinate sites in the feature space, that reside inside of the square spanned by the current extent.

#+begin_src python :tangle hypotheses/hypothesis_3.py
    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    sites = [(x,y) for x in range(extents[i]) for y in range(extents[i])]
    sites = np.array(sites, dtype=np.float32)
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')
#+end_src

Then, everything else follows the form of previous hypothesis: generate an index for exhaustive search, compute the optimal solutions, write all relevant data to file.

#+begin_src python :tangle hypotheses/hypothesis_3.py
    query = np.random.uniform(0, extents[i], (1, n_dims))
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims)
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')

    time_start = time.perf_counter()
    distance, solution = index.search(query, extents[i] ** 2)
    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')

    k_nearest = solution[0][:k]
    ranks = solution[0]
    k_nearest_distances = distance[0][:k]

    def invert(l):
        new_l = [0 for i in range(len(l))]

        for index, value in enumerate(l):
            new_l[value] = index

        return new_l

    ranks = invert(ranks)

    instance = file.create_dataset('instance_' + str(i), data=sites)
    instance.attrs['n_sites'] = extents[i] ** 2
    instance.attrs['n_dims'] = n_dims
    instance.attrs['n_planes'] = n_dims * 2
    file.create_dataset('query_' + str(i), data=query)
    file.create_dataset('solution_' + str(i), data=k_nearest)
    file.create_dataset('ranks_' + str(i), data = ranks)
    file.create_dataset('distance_' + str(i), data=k_nearest_distances)
#+end_src

* Hypothesis 4

Hypothesis: The observed loss of quality in hypothesis 3 can be counteracted by increasing the number of separating hyperplanes.

Method of Evaluation: Consider a uniform grid, akin to hypothesis 3, but this time with a fixed extent. Successive instances increase the number of separating hyperplanes.


#+begin_src python :tangle hypotheses/hypothesis_4.py
#!/usr/bin/env python3

# Hypothesis 4: The observed loss of quality in hypothesis 3 can be counteracted by increasing the number of separating hyperplanes.


import time
import numpy as np
import h5py
import faiss

n_dims = 2
k = 5
extent = 100
n_planes = [i * n_dims for i in range(20)]
filepath = "data/hypothesis_4"

hypothesis = "The observed loss of quality in hypothesis 3 can be counteracted by increasing the number of separating hyperplanes."

description = """
Consider a uniform grid, akin to hypothesis 3, but this time with a fixed extent. Successive instances increase the number of separating hyperplanes.
"""

np.random.seed(42)

file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_instances'] = len(n_planes)
file.attrs['description'] = description
file.attrs['hypothesis'] = hypothesis

for i in range(len(n_planes)):

    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    sites = [(x,y) for x in range(extent) for y in range(extent)]
    sites = np.array(sites, dtype=np.float32)
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')

    query = np.random.uniform(0, extent, (1, n_dims))
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims)
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')

    time_start = time.perf_counter()
    distance, solution = index.search(query, extent ** 2)
    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')

    k_nearest = solution[0][:k]
    ranks = solution[0]
    k_nearest_distances = distance[0][:k]

    def invert(l):
        new_l = [0 for i in range(len(l))]

        for index, value in enumerate(l):
            new_l[value] = index

        return new_l

    ranks = invert(ranks)


    instance = file.create_dataset('instance_' + str(i), data=sites)
    instance.attrs['n_sites'] = extent ** 2
    instance.attrs['n_dims'] = n_dims
    instance.attrs['n_planes'] = n_planes[i]
    file.create_dataset('query_' + str(i), data=query)
    file.create_dataset('solution_' + str(i), data=k_nearest)
    file.create_dataset('ranks_' + str(i), data = ranks)
    file.create_dataset('distance_' + str(i), data=k_nearest_distances)
#+end_src

* Hypothesis 5

Hypothesis: Over a uniform-grid similar to hypotheses 3 & 4, the quality of LSH diminishes as the dimensionality of the feature space increases over a bounded region.

Method of evaluation: Successive instances share the extent of the bounded region inhabited by the set of sites. However, the dimensionality increments for successive instances.

Note that this hypothesis requires the library `itertools` in order to generate all sites of the uniform grid for variable dimensions.

#+begin_src python :tangle hypotheses/hypothesis_5.py
#!/usr/bin/env python3

# Hypothesis 5: Over a uniform-grid similar to hypotheses 3 & 4, the quality of LSH diminishes as the dimensionality of the feature space increases over a bounded region.

import itertools
import time
import numpy as np
import h5py
import faiss

n_dims = [i for i in range(2, 11)]
k = 5
extent = 5
filepath = "data/hypothesis_5"

hypothesis = "Over a uniform-grid similar to hypotheses 3 & 4, the quality of LSH diminishes as the dimensionality of the feature space increases."

description = """
Consider a uniform grid, akin to hypothesis 3 & 4, but this time the extent is fixed and the dimensionality increase for successive instances.
"""

np.random.seed(42)

file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_instances'] = len(n_dims)
file.attrs['description'] = description
file.attrs['hypothesis'] = hypothesis

for i in range(len(n_dims)):

    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    sites = [s for s in itertools.product(range(extent), repeat=n_dims[i])]
    sites = np.array(sites, dtype=np.float32)
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')

    query = np.random.uniform(0, extent, (1, n_dims[i]))
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims[i])
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')

    time_start = time.perf_counter()
    distance, solution = index.search(query, extent ** n_dims[i])
    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')

    k_nearest = solution[0][:k]
    ranks = solution[0]
    k_nearest_distances = distance[0][:k]

    def invert(l):
        new_l = [0 for i in range(len(l))]

        for index, value in enumerate(l):
            new_l[value] = index

        return new_l

    ranks = invert(ranks)


    instance = file.create_dataset('instance_' + str(i), data=sites)
    instance.attrs['n_sites'] = extent ** n_dims[i]
    instance.attrs['n_dims'] = n_dims[i]
    instance.attrs['n_planes'] = n_dims[i] * 2
    file.create_dataset('query_' + str(i), data=query)
    file.create_dataset('solution_' + str(i), data=k_nearest)
    file.create_dataset('ranks_' + str(i), data = ranks)
    file.create_dataset('distance_' + str(i), data=k_nearest_distances)
#+end_src

* Hypothesis 6

Hypothesis: Quality of LSH queries degenerates as density of sites increases.

Method of evaluation: We generate a uniformly-distributed set of sites inside of the unit hypercube centered around the origin. Successive instances increase the number of sites linearly.

#+begin_src python :tangle hypotheses/hypothesis_6.py
#!/usr/bin/env python3

# Hypothesis 6: Quality of LSH queries degenerates as density of sites increases.


import itertools
import time
import numpy as np
import h5py
import faiss

n_sites = [100, 1000, 5000, 10000, 50000, 100000, 500000, 100000]
n_dims = [100 for i in range(len(n_sites))]
k = 5
filepath = "data/hypothesis_6"

hypothesis = "Quality of LSH queries degenerates as density of sites increases."

description = """
We generate a uniformly-distributed set of sites inside of the unit hypercube centered around the origin. Successive instances increase the number of sites linearly.
"""

np.random.seed(42)

file = h5py.File(filepath, 'w')
file.attrs['k'] = k
file.attrs['n_instances'] = len(n_dims)
file.attrs['description'] = description
file.attrs['hypothesis'] = hypothesis

for i in range(len(n_dims)):

    print(f'Generating instance {i}:')
    time_start = time.perf_counter()
    sites = np.random.uniform(-0.5, 0.5, (n_sites[i], n_dims[i]))
    time_end = time.perf_counter()
    print(f'\tGenerating sites: {time_end - time_start:.3f} seconds')

    query = np.random.uniform(-0.5, 0.5, (1, n_dims[i]))
    time_start = time.perf_counter()
    index = faiss.IndexFlatL2(n_dims[i])
    index.add(sites)
    time_end = time.perf_counter()
    print(f'\tGenerating flat index: {time_end - time_start:.3f} seconds')

    time_start = time.perf_counter()
    distance, solution = index.search(query, n_sites[i])
    time_end = time.perf_counter()
    print(f'\tComputing solution: {time_end - time_start:.3f} seconds')

    k_nearest = solution[0][:k]
    ranks = solution[0]
    k_nearest_distances = distance[0][:k]

    def invert(l):
        new_l = [0 for i in range(len(l))]

        for index, value in enumerate(l):
            new_l[value] = index + 1

        return new_l

    ranks = invert(ranks)


    instance = file.create_dataset('instance_' + str(i), data=sites)
    instance.attrs['n_sites'] = n_sites[i]
    instance.attrs['n_dims'] = n_dims[i]
    instance.attrs['n_planes'] = n_dims[i] * 2
    file.create_dataset('query_' + str(i), data=query)
    file.create_dataset('solution_' + str(i), data=k_nearest)
    file.create_dataset('ranks_' + str(i), data = ranks)
    file.create_dataset('distance_' + str(i), data=k_nearest_distances)
#+end_src
