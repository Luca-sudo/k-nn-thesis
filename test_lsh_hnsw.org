#+title: Test LSH & HNSW

This script forms the backbone for comparing the quality of HNSW and LSH on instances.
It receives a filepath to the instance data as its sole argument.
Then, it reads the file and processes it.
Before we can do this, we first have to import any necessary library. To this end we use:
- Matplotlib to generate graphs of the benchmark-data
- Csv to write the benchmark data to a csv file
- sys to extract CLI arguments -- in this case the path to the data
- time to measure index creation and query times
- numpy to work with arrays
- h5py to read previously generated data
- faiss to create HNSW and LSH indexes.

#+begin_src python :tangle test_lsh_hnsw.py
#!/usr/bin/env python3
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import sys
import time
import numpy as np
import h5py
import faiss
#+end_src

With all libraries in place, we have to extract the filepath to the data from the CLI.

#+begin_src python :tangle test_lsh_hnsw.py
if(len(sys.argv) <= 1):
    raise Exception("Please supply a filepath for data to compare!")

filepath = sys.argv[1]
#+end_src

We then read the first parameters of the test data -- such as the number of instances, description and $k$ for nearest neighbour queries -- before printing an explanation of the hypothesis this data examines as well as a natural language description of the data.

#+begin_src python :tangle test_lsh_hnsw.py
file = h5py.File(filepath, 'r')
k = file.attrs['k'].item()
n_instances = file.attrs['n_instances']
description = file.attrs['description']
hypothesis = file.attrs['hypothesis']
sample_size = file.attrs['sample_size']


print(f'Hypothesis: {hypothesis}')
print(f'Description: {description}')
#+end_src

Before testing each instance, we setup lists for all data that we are interested in. We initialize these lists with a short, descriptive string; this lets us write the data to a .csv file descriptively later on.

#+begin_src python :tangle test_lsh_hnsw.py
timings = []
qualities = []
ranks = []
#+end_src

Finally, we begin evaluating all instances. For this we extract the data, which includes the dimensionality, amount of sites, optimal solutions to the k-nearest neighbour query, etc.

#+begin_src python :tangle test_lsh_hnsw.py
for i in range(n_instances):
    print(f'Extracting instance {i}.')
    time_start = time.perf_counter()
    dataset = file['instance_' + str(i)]
    n_dim = dataset.attrs['n_dims'].item()
    n_sites = dataset.attrs['n_sites']
    n_planes = dataset.attrs['n_planes'].item()
    print('n_planes: ', n_planes)
    sites = dataset[()]
    optimal_distances = file['distance_' + str(i)][()]
    site_to_rank = file['ranks_' + str(i)][()]
    queries = file['queries_' + str(i)][()]
    solution = file['solution_' + str(i)][()]
    time_end = time.perf_counter()
    dt = round(time_end - time_start, 7)
    print(f'\tExtracting data: \t{dt:.3f} seconds')
#+end_src

With the instance data at hand, we now create the HNSW index and time it.

#+begin_src python :tangle test_lsh_hnsw.py
    time_start = time.perf_counter()
    hnsw_index = faiss.IndexHNSWFlat(n_dim, 4)
    hnsw_index.hnsw.efConstruction = 200
    hnsw_index.add(sites)
    time_end = time.perf_counter()
    dt = round(time_end - time_start, 7)
    print(f'\tCreating HNSW Index: \t{time_end - time_start:.3f} seconds')
    timings.append({
        'instance' : i,
        'algo' : 'hnsw',
        'event' : 'creation',
        'dt' : dt
    })
#+end_src

We then query this HNSW index.

#+begin_src python :tangle test_lsh_hnsw.py
    time_start = time.perf_counter()
    hnsw_distance, hnsw_solutions = hnsw_index.search(queries, k)
    time_end = time.perf_counter()
    dt = round(time_end - time_start, 7)
    print(f'\tQuerying HNSW Index: \t{time_end - time_start:.3f} seconds')
    timings.append({
        'instance' : i,
        'algo' : 'hnsw',
        'event' : 'query',
        'dt' : dt / sample_size
    })
#+end_src

Afterwards, we create the LSH index and query it too.

#+begin_src python :tangle test_lsh_hnsw.py
    time_start = time.perf_counter()
    lsh_index = faiss.IndexLSH(n_dim, n_planes)
    lsh_index.add(sites)
    time_end = time.perf_counter()
    dt = round(time_end - time_start, 7)
    print(f'\tCreating LSH Index: \t{time_end - time_start:.3f} seconds')
    timings.append({
        'instance' : i,
        'algo' : 'lsh',
        'event' : 'creation',
        'dt' : dt
    })

    time_start = time.perf_counter()
    _, lsh_solutions = lsh_index.search(queries, k)
    time_end = time.perf_counter()
    dt = round(time_end - time_start, 7)
    print(f'\tQuerying LSH Index: \t{time_end - time_start:.3f} seconds')
    timings.append({
        'instance' : i,
        'algo' : 'lsh',
        'event' : 'query',
        'dt' : dt / sample_size
    })

    print(f'\tLSH Solutions: {lsh_solutions}')
    print(f'\tHNSW Solutions: {hnsw_solutions}')
#+end_src

Before we can evaluate the quality of the LSH index, we have to calculate the distances for candidate solutions manually. While the HNSW index returned euclidean distances together with the set of candidate solutions, LSH returns the hamming distance instead. We, therefore, calculate the euclidean distances for all LSH candidate solutions.

#+begin_src python :tangle test_lsh_hnsw.py
    solution_vectors = np.array(list(map(lambda x : sites[x], lsh_solutions)))
    lsh_distance = []
    for sample_idx in range(sample_size):
        print(sample_idx)
        sample = solution_vectors[sample_idx, :, :]
        distances = sorted(np.sum((sample - queries[sample_idx]) ** 2, axis=1))
        lsh_distance.append(distances)
#+end_src

Next, we sort the resulting distances of both HNSW and LSH from best to worst, before we calculate the quality of the respective solutions. We define the quality of a candidate solution relative to the optimal solution through the formula $\frac{\text{Optimal Distance}}{\text{Achieved Distance}}$. Hence, the quality of a candidate is $1$ if it is precisely the optimal solution. In any other case, the quality approaches zero as the realized distance worsens.

#+begin_src python :tangle test_lsh_hnsw.py
    sorted_lsh_distances = lsh_distance
    sorted_hnsw_distances = list(map(lambda x : sorted(x), hnsw_distance))
    print(f'hnsw_distance: {hnsw_distance}')
    print(f'hnsw_sorted_distance: {sorted_hnsw_distances}')

    lsh_quality = optimal_distances / sorted_lsh_distances
    print(f'lsh_quality {lsh_quality}')
    hnsw_quality = optimal_distances / sorted_hnsw_distances
    print(f'hnsw_quality {hnsw_quality}')
#+end_src

Finally, we print and store the values of interest: minimum, median and maximum quality for both HNSW and LSH.


#+begin_src python :tangle test_lsh_hnsw.py
    var_name = file.attrs['var_name']
    var_value = file.attrs['var_values'][i]

    for sample in lsh_quality:
        for q in sample:
                qualities.append({
                var_name : var_value,
                'algo' : 'lsh',
                'Quality' : q
                })

    for sample in hnsw_quality:
        for q in sample:
                qualities.append({
                var_name : var_value,
                'algo' : 'hnsw',
                'Quality' : q
                })

#+end_src

Besides the quality, the rank of a candidate solution is of interest too. It is an integer $i$ that, for a given site $s$, denotes that $s$ is the $i$-th neighbor. To accelerate determining $i$, it suffices to index into the `ranks_i` list. At position $s$ of the `ranks_i` list -- note that faiss indices return index values into the original list of sites -- the corresponding $i$ can be extracted.

#+begin_src python :tangle test_lsh_hnsw.py
    current_lsh_ranks = []
    current_hnsw_ranks = []

    for sample_idx, neighbors in enumerate(lsh_solutions):
        for r in site_to_rank[sample_idx][neighbors]:
                ranks.append({
                        var_name : var_value,
                        'algo' : 'lsh',
                        'Rank' : r
                })

    for sample_idx, neighbors in enumerate(hnsw_solutions):
        for r in site_to_rank[sample_idx][neighbors]:
                ranks.append({
                        var_name : var_value,
                        'algo' : 'hnsw',
                        'Rank' : r
                })
#+end_src

:TODO: Add section on csv and plot generation.

#+begin_src python :tangle test_lsh_hnsw.py

timings = pd.DataFrame(timings)
qualities = pd.DataFrame(qualities)
ranks = pd.DataFrame(ranks)

metadata = {
    'var_name' : file.attrs['var_name']
}

timings.to_hdf(filepath + "_results.h5", key="timings", format="table")
qualities.to_hdf(filepath + "_results.h5", key="qualities", format="table")
ranks.to_hdf(filepath + "_results.h5", key="ranks", format="table")

results = h5py.File(filepath + "_results.h5", 'r+')

results.attrs['var_name'] = file.attrs['var_name']
#+end_src

#+begin_src python :tangle plot_lsh_hnsw.py
import pandas as pd
import h5py
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys

if(len(sys.argv) <= 1):
    raise Exception("Please supply a filepath for data to plot!")

filepath, _ = os.path.splitext(sys.argv[1])

qualities = pd.read_hdf(filepath + "_results.h5", key="qualities")
timings = pd.read_hdf(filepath + "_results.h5", key="timings")
ranks = pd.read_hdf(filepath + "_results.h5", key="ranks")

results = h5py.File(filepath + "_results.h5", 'r')

var_name = results.attrs['var_name']
print(f'var_name : {var_name}')

sns.lineplot(data=qualities, x=var_name, y='Quality', hue='algo')

plt.savefig(filepath + "_qualities.pdf")

plt.clf()

sns.boxplot(data=ranks, x=var_name, y='Rank', hue='algo')

plt.yscale('log', base=2)

plt.savefig(filepath + "_ranks.pdf")

plt.clf()

sns.boxplot(data=timings, x='event', y='dt', hue='algo')

plt.savefig(filepath + "_timings.pdf")

#+end_src
